# 3.2 k近邻模型

k近邻模型有三个基本要素：距离度量，k值的选择，分类决策规则。

#### 3.2.1 模型

特征空间中，对每个训练实例点 $$x_i$$ ，距离该点比其他店更近的所有点组成的区域，称作**单元（cell）**；而k近邻模型将实例 $$x_i$$ 的类 $$y_i$$ 作为其单元内所有点的标记，称为类标记（class label）。

#### 3.2.2 距离度量

特征空间中两个实例点的距离是两个实例点相似程度的反映。一般使用 $$L_p$$ 距离（ $$L_p$$ distance）或Minkowski 距离（Minkowski distance）。

$$L_p$$ 距离定义：

$$
L_p(x_i, x_j) = (\sum^n_{l=1}|x_i^l - x_j^l|^{p})^{\frac{1}{p}}
$$

其中 $$p \geq 1$$ 。

* 当 $$p=1$$ 时，为曼哈顿距离（Manhattan distance）： $$L_1(x_i, x_j) = \sum^n_{l=1}|x_i^l - x_j^l|$$ 
* 当 $$p\geq2$$ 时，为欧氏距离（Euclidean distance）： $$L_2(x_i, x_j) = (\sum^n_{l=1}|x_i^l - x_j^l|^{2})^{\frac{1}{2}}$$ 
* 当 $$p = \infty$$ 时，它是各个坐标距离的最大值： $$L_{\infty}(x_i, x_j) = \underset{l}{max} | x_i^{(l)} - x_j^{(l)}|$$ 

#### 3.2.3 k值的选择

k值的选择会对k近邻法的结果产生重大影响。

* 越小的k值，模型越复杂，并且容易发生拟合
* 越大的k值，会减少学习的估计误差，但学习的近似误差会增大

#### 3.2.4 分类决策规则

k近邻法中的分类决策规则一般为多数表决（majority voting rule），即由输入实例的 $$k$$ 个邻近的训练实例中的多数类决定输入实例的类。

对于分类函数 $$f:R^n\rightarrow \{c_1, c_2, \dots, c_n\}$$ ，有误分类概率 $$P(Y \neq f(X)) = 1 - P(Y = f(X))$$ 。而对于 $$x \in \mathcal{X}$$ 其最近邻的k个训练实例点构成集合 $$N_k(x)$$ ，如果涵盖 $$N_k(x)$$ 的区域时类别 $$c_j$$ ，那么误分类率有：

$$
\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq c_j) = 1 - \frac{1}{k}\sum_{x_i \in N_k(x)} I(y_i = c_j)
$$

多数表决规则等于经验风险最小化。

