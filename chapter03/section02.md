# 3.2 k近邻模型

三个基本要素：距离度量，k值的选择，分类决策规则。

#### 3.2.1 模型

单元（cell）：特征空间中，对每个训练实例点 $$x_i$$ ，距离该点比其他店更近的所有点组成的区域。

类标记（class label）：将实例 $$x_i$$ 的类 $$y_i$$ 作为其单元内所有点的标记。

#### 3.2.2 距离度量

特征空间中两个实例点的距离是两个实例点相似程度的反映。

$$
Lp(x_i, x_j) = (\sum^n_{l=1}|x_i^l - x_j^l|^{p})^{\frac{1}{p}}
$$

其中 $$p \geq 1$$ ，当 $$p=1$$ 时，为曼哈顿距离（Manhattan distance）；当 $$p\geq2$$ 时，为欧氏距离（Euclidean distance）；当 $$p = \infty$$ 时，它是各个坐标距离的最大值。

#### 3.2.3 k值的选择

k值的选择会对k近邻法的结果产生重大影响。

* 越小的k值，模型越复杂，并且容易发生拟合
* 越大的k值，会减少学习的估计误差，但学习的近似误差会增大

#### 3.2.4 分类决策规则

分类决策一般为多数表决（majority voting rule）。

分类函数： $$f:R^n\rightarrow \{c_1, c_2, \dots, c_n\}$$ 

误分类概率： $$P(Y \neq f(X)) = 1 - P(Y = f(X))$$ 

对于 $$x \in \mathcal{X}$$ 其最近邻的k个训练实例点构成集合 $$N_k(x)$$ ，如果涵盖 $$N_k(x)$$ 的区域时类别 $$c_j$$ ，那么误分类率有：

$$
\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq c_j) = 1 - \frac{1}{k}\sum_{x_i \in N_k(x)} I(y_i = c_j)
$$

即多数表决规则等于经验风险最小化。

