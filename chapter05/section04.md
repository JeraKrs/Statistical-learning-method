# 5.4 决策树的剪枝

未剪枝的决策树往往对训练数据的分类很准确，但对未知的数据的分类缺不准确，即过拟合现象。

剪枝（pruning）：在决策树中，对已生成的数进行简化。

剪枝通常通过极小化决策树的损失函数（loss function）或代价函数（cost function）。

损失函数：

$$
C_{\alpha}(T) = C(T) + \alpha|T| = \sum^{|T|}_{i=1}N_tH_t(T) + \alpha|T|
$$

其中树 $$T$$ 的结点个数为 $$|T|$$ ， $$t$$ 为 $$T$$ 的叶子结点，该结点有 $$N_t$$ 个样本点，其中 $$k$$ 类的样本点有 $$N_{tk}$$ 个， $$H_t(T)$$ 为叶子结点 $$t$$ 上的经验熵， $$\alpha \geq 0$$ 为参数。

$$
H_t(T) = -\sum_k \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}
$$

较小的 $$\alpha$$ 促使选择较复杂的模型：

* 决策树的生成只考虑了通过提高信息增益对训练数据进行更好的拟合
* 决策树的剪枝通过优化损失函数，并考虑了减少模型的复杂度

**算法5-4（树的剪枝算法）**

输入：生成算法产生的整个树 $$T$$ ；参数 $$\alpha$$ 

输出：修剪后的子树 $$T_{\alpha}$$ 

1. 计算每个结点的经验熵
2. 递归地从树的叶子结点向上回缩：设一组叶结点回缩到其父结点之前和之后的整体树分别为 $$T_{B}, T_{A}$$，其对应的损失值分别为 $$C_{\alpha}(B), C_{\alpha}(A)$$ ，如果 $$C_{\alpha}(A) \leq C_{\alpha}(B)$$ ，则进行剪枝
3. 返回步\(2\)直至不能继续为止



