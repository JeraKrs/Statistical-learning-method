# 5.3 决策树的生成

#### 5.3.1 ID3算法

**算法5-2（ID3算法）**

输入：训练数据集 $$D$$ ，特征 $$A$$ ，阈值 $$\varepsilon $$ 

输出：决策树 $$T$$ 

1. 若 $$D$$ 中所有实例属于同一类 $$C_k$$ ，则 $$T$$ 为单节点树，并将类 $$a = C_k$$ 作为该结点的类标记
2. 若 $$A = \emptyset$$ ，则 $$T$$ 为单节点数，并将 $$D$$ 中实例数最大的类 $$C_k$$ 作为该结点的类标记
3. 否则按照算法5-1计算 $$A$$ 中各个特征对 $$D$$ 的信息增益，选择信息增益最大的特征 $$A_g$$ 
4. 如果 $$A_g$$ 的信息增益小于阈值 $$\varepsilon$$ ，则置 $$T$$ 为单结点树，并将 $$D$$ 中实例数最大的类 $$C_k$$ 作为该结点的类标记
5. 否则，对 $$A_g$$ 的每一个可能值 $$a_{i}$$ ，依 $$A_g = a_i$$ 将 $$D$$ 分割为若干非空子集 $$D_i$$ ，将 $$D_i$$ 中实例数最大的类 $$C_k$$ 作为该结点的类标记，构建子节点，并由结点和其子节点构成子决策树
6. 对第 $$i$$ 个子节点，以 $$D_i$$ 为训练集，以 $$A - \{A_g\}$$ 为特征集，递归调用步\(1\)~\(5\)

#### 5.3.2 C4.5的生成算法

**算法5-3（C4.5算法）**

1. 若 $$D$$ 中所有实例属于同一类 $$C_k$$ ，则 $$T$$ 为单节点树，并将类 $$a = C_k$$ 作为该结点的类标记
2. 若 $$A = \emptyset$$ ，则 $$T$$ 为单节点数，并将 $$D$$ 中实例数最大的类 $$C_k$$ 作为该结点的类标记
3. 否则按照算法5-1计算 $$A$$ 中各个特征对 $$D$$ 的信息增益，选择信息增比最大的特征 $$A_g$$ 
4. 如果 $$A_g$$ 的信息增益小于阈值 $$\varepsilon$$ ，则置 $$T$$ 为单结点树，并将 $$D$$ 中实例数最大的类 $$C_k$$ 作为该结点的类标记
5. 否则，对 $$A_g$$ 的每一个可能值 $$a_{i}$$ ，依 $$A_g = a_i$$ 将 $$D$$ 分割为若干非空子集 $$D_i$$ ，将 $$D_i$$ 中实例数最大的类 $$C_k$$ 作为该结点的类标记，构建子节点，并由结点和其子节点构成子决策树
6. 对第 $$i$$ 个子节点，以 $$D_i$$ 为训练集，以 $$A - \{A_g\}$$ 为特征集，递归调用步\(1\)~\(5\)



